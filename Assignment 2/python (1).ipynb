{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction To Data Science – Assignment 2\n",
        "\n",
        "---\n",
        "\n",
        "#### Sections A – B – C – D\n",
        "\n",
        "---\n",
        "\n",
        "##***Instructions: Read These Carefully Before Starting!***\n",
        "\n",
        "1. Due Date: Thursday 20th October 2022 – 11:59PM\n",
        "\n",
        "2. **Name the file in the format Lyyxxxx_A2.ipynb and save it as .ipynb (e.g. L216666_A2.ipynb)**\n",
        "\n",
        "3. Submission will be taken on Google Classroom (**submit SINGLE .ipynb file ONLY**)\n",
        "\n",
        "4. **Assignment will not be evaluated if**:\n",
        "\n",
        "> * You submit python (.py) files\n",
        "> * You submit multiple .ipynb files\n",
        "> * You submit compressed (.rar or .zip) files\n",
        "\n",
        "5. **Work in the spaces provided and do not delete/modify any cells from this template.**\n",
        "\n",
        "6. Upload data files directly to Google Colab - do not use Google Drive or GitHub linking method\n",
        "\n",
        "*Not following these instructions will lead to mark deduction.*\n",
        "\n",
        "---\n",
        "\n",
        "All source files needed to complete this assignment can be found on the following [Google Drive link](https://drive.google.com/drive/folders/1qBib_6ZOhvHb73ZRLWiCMWl9NFyU1IDO?usp=sharing). Download these files and upload them to your Google Colab Notebook. \n",
        "\n",
        "**Do not link Google Drive or GitHub with Colab.**\n",
        "\n",
        "**Do not add these files with your submission on Google Classroom.**\n",
        "\n",
        "---\n",
        "\n",
        "Happy Coding 🌺\n",
        "\n",
        "---\n",
        "\n",
        "TA Emails\n",
        "\n",
        "Section A, C - Muhammad Maarij l192347@lhr.nu.edu.pk\n",
        "\n",
        "Section B, D - Hira Ijaz l192377@lhr.nu.edu.pk\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TWWLTwYZorzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Question 0\n",
        "\n",
        "Add all library imports here"
      ],
      "metadata": {
        "id": "mwtAOew56gkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "swlE8Tvq6g2u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR8C9ctqsy3V"
      },
      "source": [
        "---\n",
        "## Question 1\n",
        "\n",
        "####Single Linear Regression with Gradient Descent\n",
        "\n",
        "> Take help from slides 26 and 30\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzGOilP7sy3Y"
      },
      "source": [
        "**Part A -**\n",
        "Write a function that calculates and returns value for hypothesis $h_\\theta(x)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "id": "3bczSIhosy3Z"
      },
      "outputs": [],
      "source": [
        "# complete this function implementation\n",
        "def hypothesis(x, theta):\n",
        "  hypothesis_x=theta[0]+theta[1]*x\n",
        "  return hypothesis_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXORHsxSsy3a"
      },
      "source": [
        "**Part B -**\n",
        "Write a function that calculates and returns value for loss/cost $J(\\theta_0, \\theta_1)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "id": "__DPftoLsy3c"
      },
      "outputs": [],
      "source": [
        "# complete this function implementation\n",
        "def loss(hypothesis_x, y):\n",
        "   slope =len(hypothesis_x)\n",
        "   J=np.sum((hypothesis_x -y)**2)*(1/2 * slope)\n",
        "   return J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjWa6pMGsy3d"
      },
      "source": [
        "**Part C-**\n",
        "Write a function that applies the gradient descent algorithm and updates values of $\\theta_0$ and $\\theta_1$ until they converge.\n",
        "\n",
        "* take default vaue of $α$ to be 0.015\n",
        "* take default number of iterations to be 15000\n",
        "* print loss after every 500 iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": true,
        "id": "U7qtzuCWsy3e"
      },
      "outputs": [],
      "source": [
        "# complete this function implementation\n",
        "def gradientDescent(x, y, theta, numIterations=15000, alpha=0.015):\n",
        "    slope=len(x)\n",
        "    count =0\n",
        "    loss=[]\n",
        "    while(count < numIterations):\n",
        "        y_pred= hypothesis(x,theta)\n",
        "        theta[0]= theta[0] - (alpha)*((1/slope)* sum(y_pred - y))\n",
        "        theta[1]= theta[1] - (alpha)*((1/slope)* (np.sum(y_pred - y)*x))\n",
        "        count =count +1\n",
        "        if(count % 500 == 0):\n",
        "            Loss= loss(y_pred , y)\n",
        "            print(Loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zwvS5cqsy3g"
      },
      "source": [
        "**Part D -**\n",
        "FactoryRevenue.csv contains information about the number of workers in a factory and the annual profit for that factory. Import the file FactoryRevenue.csv as a Pandas DataSet and print out the information for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": true,
        "id": "Uef-aGHisy3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83067722-bb62-4566-86d1-6396f274a96e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    TotalFactoryWorkers  AnnualProfit\n",
            "0                6.5894       5.99660\n",
            "1                9.2482      12.13400\n",
            "2                5.8918       1.84950\n",
            "3                8.2111       6.54260\n",
            "4                7.9334       4.56230\n",
            "5                8.0959       4.11640\n",
            "6                5.6063       3.39280\n",
            "7               12.8360      10.11700\n",
            "8                6.3534       5.49740\n",
            "9                6.3328       1.42330\n",
            "10               6.3589      -1.42110\n",
            "11               6.2742       2.47560\n",
            "12               5.6397       4.60420\n",
            "13               9.3102       3.96240\n",
            "14               9.4536       5.41410\n",
            "15               8.8254       5.16940\n",
            "16               5.1793      -0.74279\n",
            "17              21.2790      17.92900\n",
            "18              14.9080      12.05400\n",
            "19              18.9590      17.05400\n",
            "20               7.2182       4.88520\n",
            "21               8.2951       5.74420\n",
            "22              10.2360       7.77540\n",
            "23               5.4994       1.01730\n",
            "24              20.3410      20.99200\n",
            "25              10.1360       6.67990\n",
            "26               6.1101      17.59200\n",
            "27               5.5277       9.13020\n",
            "28               8.5186      13.66200\n",
            "29               7.0032      11.85400\n",
            "30               5.8598       6.82330\n",
            "31               8.3829      11.88600\n",
            "32               7.4764       4.34830\n",
            "33               8.5781      12.00000\n",
            "34               6.4862       6.59870\n",
            "35               5.0546       3.81660\n",
            "36               5.7107       3.25220\n",
            "37              14.1640      15.50500\n",
            "38               5.7340       3.15510\n",
            "39               8.4084       7.22580\n",
            "40               5.6407       0.71618\n",
            "41               5.3794       3.51290\n",
            "42               6.3654       5.30480\n",
            "43               5.1301       0.56077\n",
            "44               6.4296       3.65180\n",
            "45               7.0708       5.38930\n",
            "46               6.1891       3.13860\n",
            "47              20.2700      21.76700\n",
            "48               5.4901       4.26300\n",
            "49               6.3261       5.18750\n",
            "50               5.5649       3.08250\n",
            "51              18.9450      22.63800\n",
            "52              12.8280      13.50100\n",
            "53              10.9570       7.04670\n",
            "54              13.1760      14.69200\n",
            "55              22.2030      24.14700\n",
            "56               5.2524      -1.22000\n",
            "57               9.1802       6.79810\n",
            "58               6.0020       0.92695\n",
            "59               5.5204       0.15200\n",
            "60               5.0594       2.82140\n",
            "61               5.7077       1.84510\n",
            "62               7.6366       4.29590\n",
            "63               5.8707       7.20290\n",
            "64                  NaN       4.35650\n",
            "65               5.3054       1.98690\n",
            "66               8.2934       0.14454\n",
            "67              13.3940       9.05510\n",
            "68               5.4369       0.61705\n",
            "69               7.3345       4.02590\n",
            "70               6.0062       1.27840\n",
            "71               7.2259       3.34110\n",
            "72               5.0269      -2.68070\n",
            "73               6.5479       0.29678\n",
            "74               7.5386       3.88450\n",
            "75               5.0365       5.70140\n",
            "76              10.2740       6.75260\n",
            "77               5.1077       2.05760\n",
            "78               5.7292       0.47953\n",
            "79               5.1884       0.20421\n",
            "80               6.3557       0.67861\n",
            "81               9.7687       7.54350\n",
            "82               6.5159       5.34360\n",
            "83               8.5172       4.24150\n",
            "84               5.4069       0.55657\n",
            "85               6.8825       3.91150\n",
            "86              11.7080       5.38540\n",
            "87               5.7737       2.44060\n",
            "88               7.8247       6.73180\n",
            "89               7.0931       1.04630\n",
            "90               1.2546           NaN\n",
            "91               5.0702       5.13370\n",
            "92               5.8014       1.84400\n",
            "93              11.7000       8.00430\n",
            "94               5.5416       1.01790\n",
            "95               7.5402       6.75040\n",
            "96               5.3077       1.83960\n",
            "97               7.4239       4.28850\n",
            "98               7.6031       4.99810\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('Update_FactoryRevenue.csv')\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv9_dDiw65o8"
      },
      "source": [
        "**Part E -**\n",
        "Remove rows that have any null values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data= pd.DataFrame(data,index=None)\n",
        "data= data.dropna()\n",
        "total_rows=len(data.axes[0])\n",
        "print(\"total number of rows after null rows: \",total_rows)\n",
        "pd.set_option(\"display.max_rows\",None , \"display.max_columns\",None)\n",
        "print(\" data frame after null rows: \\n\",data)\n"
      ],
      "metadata": {
        "id": "l50u9HZx7H5t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d4fea0-5509-412f-91dd-f02b384ab8bd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of rows after null rows:  97\n",
            " data frame after null rows: \n",
            "     TotalFactoryWorkers  AnnualProfit\n",
            "0                6.5894       5.99660\n",
            "1                9.2482      12.13400\n",
            "2                5.8918       1.84950\n",
            "3                8.2111       6.54260\n",
            "4                7.9334       4.56230\n",
            "5                8.0959       4.11640\n",
            "6                5.6063       3.39280\n",
            "7               12.8360      10.11700\n",
            "8                6.3534       5.49740\n",
            "9                6.3328       1.42330\n",
            "10               6.3589      -1.42110\n",
            "11               6.2742       2.47560\n",
            "12               5.6397       4.60420\n",
            "13               9.3102       3.96240\n",
            "14               9.4536       5.41410\n",
            "15               8.8254       5.16940\n",
            "16               5.1793      -0.74279\n",
            "17              21.2790      17.92900\n",
            "18              14.9080      12.05400\n",
            "19              18.9590      17.05400\n",
            "20               7.2182       4.88520\n",
            "21               8.2951       5.74420\n",
            "22              10.2360       7.77540\n",
            "23               5.4994       1.01730\n",
            "24              20.3410      20.99200\n",
            "25              10.1360       6.67990\n",
            "26               6.1101      17.59200\n",
            "27               5.5277       9.13020\n",
            "28               8.5186      13.66200\n",
            "29               7.0032      11.85400\n",
            "30               5.8598       6.82330\n",
            "31               8.3829      11.88600\n",
            "32               7.4764       4.34830\n",
            "33               8.5781      12.00000\n",
            "34               6.4862       6.59870\n",
            "35               5.0546       3.81660\n",
            "36               5.7107       3.25220\n",
            "37              14.1640      15.50500\n",
            "38               5.7340       3.15510\n",
            "39               8.4084       7.22580\n",
            "40               5.6407       0.71618\n",
            "41               5.3794       3.51290\n",
            "42               6.3654       5.30480\n",
            "43               5.1301       0.56077\n",
            "44               6.4296       3.65180\n",
            "45               7.0708       5.38930\n",
            "46               6.1891       3.13860\n",
            "47              20.2700      21.76700\n",
            "48               5.4901       4.26300\n",
            "49               6.3261       5.18750\n",
            "50               5.5649       3.08250\n",
            "51              18.9450      22.63800\n",
            "52              12.8280      13.50100\n",
            "53              10.9570       7.04670\n",
            "54              13.1760      14.69200\n",
            "55              22.2030      24.14700\n",
            "56               5.2524      -1.22000\n",
            "57               9.1802       6.79810\n",
            "58               6.0020       0.92695\n",
            "59               5.5204       0.15200\n",
            "60               5.0594       2.82140\n",
            "61               5.7077       1.84510\n",
            "62               7.6366       4.29590\n",
            "63               5.8707       7.20290\n",
            "65               5.3054       1.98690\n",
            "66               8.2934       0.14454\n",
            "67              13.3940       9.05510\n",
            "68               5.4369       0.61705\n",
            "69               7.3345       4.02590\n",
            "70               6.0062       1.27840\n",
            "71               7.2259       3.34110\n",
            "72               5.0269      -2.68070\n",
            "73               6.5479       0.29678\n",
            "74               7.5386       3.88450\n",
            "75               5.0365       5.70140\n",
            "76              10.2740       6.75260\n",
            "77               5.1077       2.05760\n",
            "78               5.7292       0.47953\n",
            "79               5.1884       0.20421\n",
            "80               6.3557       0.67861\n",
            "81               9.7687       7.54350\n",
            "82               6.5159       5.34360\n",
            "83               8.5172       4.24150\n",
            "84               5.4069       0.55657\n",
            "85               6.8825       3.91150\n",
            "86              11.7080       5.38540\n",
            "87               5.7737       2.44060\n",
            "88               7.8247       6.73180\n",
            "89               7.0931       1.04630\n",
            "91               5.0702       5.13370\n",
            "92               5.8014       1.84400\n",
            "93              11.7000       8.00430\n",
            "94               5.5416       1.01790\n",
            "95               7.5402       6.75040\n",
            "96               5.3077       1.83960\n",
            "97               7.4239       4.28850\n",
            "98               7.6031       4.99810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0Y68nh87HJX"
      },
      "source": [
        "**Part F -**\n",
        "\n",
        "First identify the independant and dependant variables. \n",
        "\n",
        "Then create two arrays named x and y and add independant variable data to array x, dependant variable data to array y."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# independant variable: total factory workers(x)\n",
        "# dependant variable:annual profit y\n",
        "X= data[\"TotalFactoryWorkers\"]\n",
        "Y= data[\"AnnualProfit\"]"
      ],
      "metadata": {
        "id": "m49bbkSy7xxp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vRXmn9z8H4q"
      },
      "source": [
        "**Part G -** \n",
        "\n",
        "Create an array called 'theta' that will hold $θ_0$ and $θ_1$. Initalize both values to 0.\n",
        "\n",
        "Then call the gradientDescent function using array x, array y, and array theta. Do not provide any other input parameters.\n",
        "\n",
        "Print out the values of y-intercept and slope/gradient"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "theta=[0,0]\n",
        "print(\"cost: \")\n",
        "gradientDescent(X , Y, theta ,15000 ,0.015)\n",
        "slope  = theta[1]\n",
        "intercept = theta[0]\n",
        "print(\"slope: \", slope)\n",
        "print(\"intercept: \",intercept)\n"
      ],
      "metadata": {
        "id": "e7nBeh3b9KXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f262d59-e595-4ea5-a656-0c70d1857963"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost: \n",
            "slope:  0\n",
            "intercept:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lksKYteB9NOx"
      },
      "source": [
        "**Part H -** Plot a scatter plot and regression line on the same graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(data['TotalFactoryWorkers'],data['AnnualProfit'])\n",
        "plt.plot(data['TotalFactoryWorkers'], theta[1]*X+ theta[0] , color=\"green\")\n",
        "plt.xlabel(\"Total Factory Workers\")\n",
        "plt.ylabel(\"Annual Profit\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jdah8R6L9XSl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "38414c95-a6cb-471b-a9ef-28e4c7af6bb1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Question 2\n",
        "\n",
        "Logistic Regression on Flowers Dataset\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "upsln2T-38-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part A** - Load the file FlowersData.csv and describe the dataset"
      ],
      "metadata": {
        "id": "plh_l8AV4MU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "iris_dataset = pd.read_csv(\"FlowersData.csv\")\n",
        "iris_dataset.info"
      ],
      "metadata": {
        "id": "p7Cgq8di3771",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5fe1f2d-5eed-4af9-9619-3d0e135c378a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.info of      sepal_length  sepal_width  petal_length  petal_width flower_name\n",
              "0             5.1          3.5           1.4          0.2    hibiscus\n",
              "1             4.9          3.0           1.4          0.2    hibiscus\n",
              "2             4.7          3.2           1.3          0.2    hibiscus\n",
              "3             4.6          3.1           1.5          0.2    hibiscus\n",
              "4             5.0          3.6           1.4          0.2    hibiscus\n",
              "5             5.4          3.9           1.7          0.4    hibiscus\n",
              "6             4.6          3.4           1.4          0.3    hibiscus\n",
              "7             5.0          3.4           1.5          0.2    hibiscus\n",
              "8             4.4          2.9           1.4          0.2    hibiscus\n",
              "9             4.9          3.1           1.5          0.1    hibiscus\n",
              "10            5.4          3.7           1.5          0.2    hibiscus\n",
              "11            4.8          3.4           1.6          0.2    hibiscus\n",
              "12            4.8          3.0           1.4          0.1    hibiscus\n",
              "13            4.3          3.0           1.1          0.1    hibiscus\n",
              "14            5.8          4.0           1.2          0.2    hibiscus\n",
              "15            5.7          4.4           1.5          0.4    hibiscus\n",
              "16            5.4          3.9           1.3          0.4    hibiscus\n",
              "17            5.1          3.5           1.4          0.3    hibiscus\n",
              "18            5.7          3.8           1.7          0.3    hibiscus\n",
              "19            5.1          3.8           1.5          0.3    hibiscus\n",
              "20            5.4          3.4           1.7          0.2    hibiscus\n",
              "21            5.1          3.7           1.5          0.4    hibiscus\n",
              "22            4.6          3.6           1.0          0.2    hibiscus\n",
              "23            5.1          3.3           1.7          0.5    hibiscus\n",
              "24            4.8          3.4           1.9          0.2    hibiscus\n",
              "25            5.0          3.0           1.6          0.2    hibiscus\n",
              "26            5.0          3.4           1.6          0.4    hibiscus\n",
              "27            5.2          3.5           1.5          0.2    hibiscus\n",
              "28            5.2          3.4           1.4          0.2    hibiscus\n",
              "29            4.7          3.2           1.6          0.2    hibiscus\n",
              "30            4.8          3.1           1.6          0.2    hibiscus\n",
              "31            5.4          3.4           1.5          0.4    hibiscus\n",
              "32            5.2          4.1           1.5          0.1    hibiscus\n",
              "33            5.5          4.2           1.4          0.2    hibiscus\n",
              "34            4.9          3.1           1.5          0.1    hibiscus\n",
              "35            5.0          3.2           1.2          0.2    hibiscus\n",
              "36            5.5          3.5           1.3          0.2    hibiscus\n",
              "37            4.9          3.1           1.5          0.1    hibiscus\n",
              "38            4.4          3.0           1.3          0.2    hibiscus\n",
              "39            5.1          3.4           1.5          0.2    hibiscus\n",
              "40            5.0          3.5           1.3          0.3    hibiscus\n",
              "41            4.5          2.3           1.3          0.3    hibiscus\n",
              "42            4.4          3.2           1.3          0.2    hibiscus\n",
              "43            5.0          3.5           1.6          0.6    hibiscus\n",
              "44            5.1          3.8           1.9          0.4    hibiscus\n",
              "45            4.8          3.0           1.4          0.3    hibiscus\n",
              "46            5.1          3.8           1.6          0.2    hibiscus\n",
              "47            4.6          3.2           1.4          0.2    hibiscus\n",
              "48            5.3          3.7           1.5          0.2    hibiscus\n",
              "49            5.0          3.3           1.4          0.2    hibiscus\n",
              "50            7.0          3.2           4.7          1.4    daffodil\n",
              "51            6.4          3.2           4.5          1.5    daffodil\n",
              "52            6.9          3.1           4.9          1.5    daffodil\n",
              "53            5.5          2.3           4.0          1.3    daffodil\n",
              "54            6.5          2.8           4.6          1.5    daffodil\n",
              "55            5.7          2.8           4.5          1.3    daffodil\n",
              "56            6.3          3.3           4.7          1.6    daffodil\n",
              "57            4.9          2.4           3.3          1.0    daffodil\n",
              "58            6.6          2.9           4.6          1.3    daffodil\n",
              "59            5.2          2.7           3.9          1.4    daffodil\n",
              "60            5.0          2.0           3.5          1.0    daffodil\n",
              "61            5.9          3.0           4.2          1.5    daffodil\n",
              "62            6.0          2.2           4.0          1.0    daffodil\n",
              "63            6.1          2.9           4.7          1.4    daffodil\n",
              "64            5.6          2.9           3.6          1.3    daffodil\n",
              "65            6.7          3.1           4.4          1.4    daffodil\n",
              "66            5.6          3.0           4.5          1.5    daffodil\n",
              "67            5.8          2.7           4.1          1.0    daffodil\n",
              "68            6.2          2.2           4.5          1.5    daffodil\n",
              "69            5.6          2.5           3.9          1.1    daffodil\n",
              "70            5.9          3.2           4.8          1.8    daffodil\n",
              "71            6.1          2.8           4.0          1.3    daffodil\n",
              "72            6.3          2.5           4.9          1.5    daffodil\n",
              "73            6.1          2.8           4.7          1.2    daffodil\n",
              "74            6.4          2.9           4.3          1.3    daffodil\n",
              "75            6.6          3.0           4.4          1.4    daffodil\n",
              "76            6.8          2.8           4.8          1.4    daffodil\n",
              "77            6.7          3.0           5.0          1.7    daffodil\n",
              "78            6.0          2.9           4.5          1.5    daffodil\n",
              "79            5.7          2.6           3.5          1.0    daffodil\n",
              "80            5.5          2.4           3.8          1.1    daffodil\n",
              "81            5.5          2.4           3.7          1.0    daffodil\n",
              "82            5.8          2.7           3.9          1.2    daffodil\n",
              "83            6.0          2.7           5.1          1.6    daffodil\n",
              "84            5.4          3.0           4.5          1.5    daffodil\n",
              "85            6.0          3.4           4.5          1.6    daffodil\n",
              "86            6.7          3.1           4.7          1.5    daffodil\n",
              "87            6.3          2.3           4.4          1.3    daffodil\n",
              "88            5.6          3.0           4.1          1.3    daffodil\n",
              "89            5.5          2.5           4.0          1.3    daffodil\n",
              "90            5.5          2.6           4.4          1.2    daffodil\n",
              "91            6.1          3.0           4.6          1.4    daffodil\n",
              "92            5.8          2.6           4.0          1.2    daffodil\n",
              "93            5.0          2.3           3.3          1.0    daffodil\n",
              "94            5.6          2.7           4.2          1.3    daffodil\n",
              "95            5.7          3.0           4.2          1.2    daffodil\n",
              "96            5.7          2.9           4.2          1.3    daffodil\n",
              "97            6.2          2.9           4.3          1.3    daffodil\n",
              "98            5.1          2.5           3.0          1.1    daffodil\n",
              "99            5.7          2.8           4.1          1.3    daffodil\n",
              "100           6.3          3.3           6.0          2.5        lily\n",
              "101           5.8          2.7           5.1          1.9        lily\n",
              "102           7.1          3.0           5.9          2.1        lily\n",
              "103           6.3          2.9           5.6          1.8        lily\n",
              "104           6.5          3.0           5.8          2.2        lily\n",
              "105           7.6          3.0           6.6          2.1        lily\n",
              "106           4.9          2.5           4.5          1.7        lily\n",
              "107           7.3          2.9           6.3          1.8        lily\n",
              "108           6.7          2.5           5.8          1.8        lily\n",
              "109           7.2          3.6           6.1          2.5        lily\n",
              "110           6.5          3.2           5.1          2.0        lily\n",
              "111           6.4          2.7           5.3          1.9        lily\n",
              "112           6.8          3.0           5.5          2.1        lily\n",
              "113           5.7          2.5           5.0          2.0        lily\n",
              "114           5.8          2.8           5.1          2.4        lily\n",
              "115           6.4          3.2           5.3          2.3        lily\n",
              "116           6.5          3.0           5.5          1.8        lily\n",
              "117           7.7          3.8           6.7          2.2        lily\n",
              "118           7.7          2.6           6.9          2.3        lily\n",
              "119           6.0          2.2           5.0          1.5        lily\n",
              "120           6.9          3.2           5.7          2.3        lily\n",
              "121           5.6          2.8           4.9          2.0        lily\n",
              "122           7.7          2.8           6.7          2.0        lily\n",
              "123           6.3          2.7           4.9          1.8        lily\n",
              "124           6.7          3.3           5.7          2.1        lily\n",
              "125           7.2          3.2           6.0          1.8        lily\n",
              "126           6.2          2.8           4.8          1.8        lily\n",
              "127           6.1          3.0           4.9          1.8        lily\n",
              "128           6.4          2.8           5.6          2.1        lily\n",
              "129           7.2          3.0           5.8          1.6        lily\n",
              "130           7.4          2.8           6.1          1.9        lily\n",
              "131           7.9          3.8           6.4          2.0        lily\n",
              "132           6.4          2.8           5.6          2.2        lily\n",
              "133           6.3          2.8           5.1          1.5        lily\n",
              "134           6.1          2.6           5.6          1.4        lily\n",
              "135           7.7          3.0           6.1          2.3        lily\n",
              "136           6.3          3.4           5.6          2.4        lily\n",
              "137           6.4          3.1           5.5          1.8        lily\n",
              "138           6.0          3.0           4.8          1.8        lily\n",
              "139           6.9          3.1           5.4          2.1        lily\n",
              "140           6.7          3.1           5.6          2.4        lily\n",
              "141           6.9          3.1           5.1          2.3        lily\n",
              "142           5.8          2.7           5.1          1.9        lily\n",
              "143           6.8          3.2           5.9          2.3        lily\n",
              "144           6.7          3.3           5.7          2.5        lily\n",
              "145           6.7          3.0           5.2          2.3        lily\n",
              "146           6.3          2.5           5.0          1.9        lily\n",
              "147           6.5          3.0           5.2          2.0        lily\n",
              "148           6.2          3.4           5.4          2.3        lily\n",
              "149           5.9          3.0           5.1          1.8        lily>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B** - Split data into training and test data using SKLearn train_test_split. Specify parameter test_size to be 25%\n",
        "\n",
        "Hint: You will be needing 4 arrays: X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "7LsiIV9n4QMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=iris_dataset.iloc[:,:4].values\n",
        "y=iris_dataset['flower_name']\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25)"
      ],
      "metadata": {
        "id": "f49YQ4GT4k5s"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part C** - Perform scaling on the X_test and X_train values using StandardScacler from SKLearn Library"
      ],
      "metadata": {
        "id": "Ez5Fa2ePDzXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "object = StandardScaler()\n",
        "x_train=object.fit_transform(x_train)\n",
        "x_test=object.transform(x_test)\n"
      ],
      "metadata": {
        "id": "97-2bUgtDzXv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part D** - Train Model using SKLearn LogisticRegression"
      ],
      "metadata": {
        "id": "E9BlQYh54lLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "LogisticRegr = LogisticRegression(random_state=82)\n",
        "LogisticRegr.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "hD_MUVu64xkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f330e8-b727-40f4-8b59-d33e0f7b6690"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(random_state=82)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part E** - Predict Labels for test split"
      ],
      "metadata": {
        "id": "-HKhBOzW4xSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=LogisticRegr.predict(x_test)\n",
        "y_pred\n"
      ],
      "metadata": {
        "id": "IqpPKB1M5G4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5cfd12d-5c4a-46b2-d445-ab6be7b39292"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['daffodil', 'hibiscus', 'lily', 'lily', 'daffodil', 'daffodil',\n",
              "       'hibiscus', 'hibiscus', 'daffodil', 'daffodil', 'hibiscus', 'lily',\n",
              "       'daffodil', 'lily', 'lily', 'daffodil', 'hibiscus', 'hibiscus',\n",
              "       'lily', 'lily', 'daffodil', 'daffodil', 'lily', 'hibiscus',\n",
              "       'daffodil', 'lily', 'lily', 'daffodil', 'lily', 'hibiscus',\n",
              "       'daffodil', 'daffodil', 'hibiscus', 'hibiscus', 'lily', 'daffodil',\n",
              "       'daffodil', 'hibiscus'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Question 3\n",
        "\n",
        "Confusion Matrix Construction\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "34It-TAowlCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part A** - Using the prediction result of logistic regression (Question 2) construct a confusion matrix using SKLearn confusion_matrix\n",
        "\n",
        "Print out this confusion matrix"
      ],
      "metadata": {
        "id": "vi_ix0XsxF8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_pred,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gB8U1-xGCXm",
        "outputId": "b1c2bb1e-1e51-4405-d76c-3d794ec5c45c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[14,  0,  1],\n",
              "       [ 0, 11,  0],\n",
              "       [ 0,  0, 12]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B** - Calculate and print Accuracy"
      ],
      "metadata": {
        "id": "JlYB9BY4xEh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import  accuracy_score\n",
        "accuracy_score(y_pred,y_test)"
      ],
      "metadata": {
        "id": "cupearHPxUKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d076fc7d-d28b-43f7-c5b4-c70e062a58ff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9736842105263158"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part C** - Calculate and print Recall"
      ],
      "metadata": {
        "id": "RvEXBB3CxUk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score , f1_score\n",
        "recall_score(y_test,y_pred,average='micro')"
      ],
      "metadata": {
        "id": "PyBufaj8xbKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a57178-438b-4ec2-92b6-528a9f8f7547"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9736842105263158"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part D** - Calculate and print Precision"
      ],
      "metadata": {
        "id": "W6BgzGjMxbfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "precision_score(y_test,y_pred,average='macro')"
      ],
      "metadata": {
        "id": "gHJg8C-1xg_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27bb1624-0f85-4891-d9a5-a69d04c1783a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9777777777777779"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part E** - Calculate and print $𝐹_1$ Score"
      ],
      "metadata": {
        "id": "6RPHz7hVxhbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test,y_pred,average='macro')"
      ],
      "metadata": {
        "id": "NmwjCpMixmeJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c396896-6f2d-4d47-af7d-e273cbd95901"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9751724137931035"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.1"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}